<div align="center">

# üî∑ VersaGraph RAG Agent

### *Enterprise-Grade Retrieval-Augmented Generation Platform*

[![Python 3.13+](https://img.shields.io/badge/Python-3.13%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.129%2B-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-1.2%2B-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white)](https://www.langchain.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-DC382D?style=for-the-badge&logo=qdrant&logoColor=white)](https://qdrant.tech/)
[![MongoDB](https://img.shields.io/badge/MongoDB-8.0-47A248?style=for-the-badge&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![Gradio](https://img.shields.io/badge/Gradio-6.6%2B-F97316?style=for-the-badge&logo=gradio&logoColor=white)](https://gradio.app/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

---

**VersaGraph RAG Agent** is a production-ready, multi-document Retrieval-Augmented Generation (RAG) system that enables intelligent, context-aware conversations over your uploaded documents. Powered by LangChain, Qdrant vector search, and MongoDB persistence, it delivers accurate, citation-backed responses with full conversation memory.

[Getting Started](#-getting-started) ‚Ä¢
[Architecture](#-architecture) ‚Ä¢
[API Reference](#-api-reference) ‚Ä¢
[Configuration](#%EF%B8%8F-configuration) ‚Ä¢
[Contributing](#-contributing)

</div>

---

## üìã Table of Contents

- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Environment Configuration](#environment-configuration)
  - [Running the Application](#running-the-application)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Configuration](#%EF%B8%8F-configuration)
- [License](#-license)

---

## ‚ú® Key Features

| Feature | Description |
|---|---|
| **Multi-Document RAG** | Upload and query across multiple PDF, DOCX, and TXT files simultaneously within a single chat session. |
| **Hybrid Memory System** | Combines short-term memory (recent MongoDB messages) with long-term semantic memory (Qdrant vector search over conversation history). |
| **Source Citations** | Every AI response includes precise citations with original filename and page number references. |
| **Multi-Provider LLM Support** | Seamlessly switch between **Groq** (cloud) and **Ollama** (local/self-hosted) LLM providers. |
| **Persona System** | Configure custom AI personas per chat session ‚Äî Academic Researcher, Legal Analyst, Financial Advisor, and more. |
| **Asynchronous Architecture** | Fully async from API to database layer, leveraging `asyncio`, `AsyncMongoClient`, and `AsyncQdrantClient`. |
| **Intelligent Question Rephrasing** | Automatically reformulates follow-up questions into standalone queries optimized for vector search. |
| **Interactive Web UI** | Professional Gradio-based frontend with session management, document upload, and real-time chat. |
| **LangServe Integration** | Exposes the RAG chain via LangServe with a built-in interactive playground for development and testing. |
| **Docker-Ready Infrastructure** | Pre-configured `docker-compose.yml` for one-command deployment of Qdrant and MongoDB. |

---

## üèó Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        CLIENT LAYER                                  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ   ‚îÇ   Gradio Web UI  ‚îÇ    ‚îÇ LangServe Client ‚îÇ                       ‚îÇ
‚îÇ   ‚îÇ   (Port 7860)    ‚îÇ    ‚îÇ  / REST Client   ‚îÇ                       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   APPLICATION LAYER (FastAPI - Port 8000)            ‚îÇ
‚îÇ                        ‚îÇ                                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ   ‚îÇ              API Router                       ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  /data/*     ‚îÇ  ‚îÇ  /chat/*             ‚îÇ   ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Upload &    ‚îÇ  ‚îÇ  Invoke, History,    ‚îÇ   ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Process     ‚îÇ  ‚îÇ  LangServe Playground‚îÇ   ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ                  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ             ‚îÇ                     ‚îÇ                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ  File Processing   ‚îÇ ‚îÇ     RAG Chain (LangChain)      ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  Pipeline          ‚îÇ ‚îÇ                                ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ                    ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  Loader ‚Üí Splitter ‚îÇ ‚îÇ  ‚îÇRephrase ‚îÇ‚Üí  ‚îÇ Retriever ‚îÇ   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  ‚Üí Embedder ‚Üí      ‚îÇ ‚îÇ  ‚îÇ  Prompt ‚îÇ   ‚îÇ(Doc+Hist) ‚îÇ   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  Vector Store      ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ                    ‚îÇ ‚îÇ                      ‚îÇ         ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ          ‚îÇ
‚îÇ                          ‚îÇ  ‚îÇ  QA Prompt + LLM ‚Üí Answer ‚îÇ ‚îÇ          ‚îÇ
‚îÇ                          ‚îÇ  ‚îÇ  (with Citations)         ‚îÇ ‚îÇ          ‚îÇ
‚îÇ                          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ          ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAYER               ‚îÇ                        ‚îÇ
‚îÇ                        ‚îÇ                    ‚îÇ                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ       MongoDB          ‚îÇ  ‚îÇ       Qdrant            ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ   (Port 27017)         ‚îÇ  ‚îÇ   (Port 6333/6334)      ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ                        ‚îÇ  ‚îÇ                         ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Chats Collection    ‚îÇ  ‚îÇ  ‚Ä¢ Document Vectors     ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Messages Collection ‚îÇ  ‚îÇ    Collection           ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Files Collection    ‚îÇ  ‚îÇ  ‚Ä¢ Chat History Vectors ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ                        ‚îÇ  ‚îÇ    Collection           ‚îÇ            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ† Tech Stack

| Layer | Technology | Purpose |
|---|---|---|
| **Backend Framework** | FastAPI | High-performance async REST API |
| **LLM Orchestration** | LangChain | Chain composition, prompt management, output parsing |
| **LLM Providers** | Groq / Ollama | Cloud-based or self-hosted language model inference |
| **Vector Database** | Qdrant | Semantic similarity search over document chunks and chat history |
| **Document Database** | MongoDB | Persistent storage for chats, messages, and file metadata |
| **Embeddings** | Ollama Embeddings (via FastEmbed) | Text-to-vector encoding for documents and queries |
| **Document Loaders** | PyMuPDF, Docx2txt, TextLoader | Multi-format document ingestion (PDF, DOCX, TXT) |
| **Text Splitting** | RecursiveCharacterTextSplitter | Intelligent chunking with configurable overlap |
| **API Serving** | LangServe | REST interface for LangChain chains with built-in playground |
| **Frontend** | Gradio | Interactive web-based chat UI with session management |
| **Containerization** | Docker Compose | Infrastructure orchestration for databases |
| **Data Validation** | Pydantic | Request/response schema validation and settings management |

---

## üöÄ Getting Started

### Prerequisites

- **Python** 3.13.5+
- **Docker** & **Docker Compose** (for Qdrant and MongoDB)
- **uv** (recommended) or **pip** for dependency management
- **Ollama** (if using local LLM/embeddings) ‚Äî [Install Ollama](https://ollama.com/)
- **Groq API Key** (if using Groq as LLM provider) ‚Äî [Get API Key](https://console.groq.com/)

### Installation

**1. Clone the repository:**

```bash
git clone https://github.com/your-username/VersaGraph-RAG-Agent.git
cd VersaGraph-RAG-Agent
```

**2. Start infrastructure services:**

```bash
docker compose up -d
```

This launches:
- **Qdrant** vector database on ports `6333` (HTTP) and `6334` (gRPC)
- **MongoDB** on port `27017`

**3. Install Python dependencies:**

```bash
# Using uv (recommended)
uv sync

# Or using pip
pip install -e .
```

**4. Pull the embedding model (if using Ollama):**

```bash
ollama pull nomic-embed-text
```

### Environment Configuration

Create a `.env` file in the project root:

```env
# ‚îÄ‚îÄ File Processing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FILE_ALLOWED_TYPES=["application/pdf","text/plain","application/vnd.openxmlformats-officedocument.wordprocessingml.document"]
FILE_MAX_SIZE_MB=50
FILE_UPLOAD_CHUNK_SIZE=131072

# ‚îÄ‚îÄ Text Chunking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CHUNK_SIZE=512
CHUNK_OVERLAP=64

# ‚îÄ‚îÄ Vector Store (Qdrant) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
VECTOR_STORE_TYPE=qdrant
EMBEDDING_MODEL=nomic-embed-text
EMBED_MODEL_SIZE=768
DISTANCE_METRIC=Cosine
URL_QDRANT=http://localhost:6333
QDRANT_API_KEY=
COLLECTION_APP_NAME=versagraph_documents
COLLECTION_CHATS_HISTORY_NAME=versagraph_chat_history

# ‚îÄ‚îÄ MongoDB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MONGODB_URL=mongodb://admin:password@localhost:27017
MONGOBD_USERNAME=admin
MONGODB_PASSWORD=password
MONGODB_DATABASE=versagraph

# ‚îÄ‚îÄ LLM Provider ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LLM_PROVIDER=groq                    # Options: "groq" or "ollama"
LLM_MODEL=llama-3.3-70b-versatile    # Model name for the chosen provider
API_KEY_GROQ=gsk_your_api_key_here   # Required if LLM_PROVIDER=groq
API_URL_LLM=http://localhost:11434   # Required if LLM_PROVIDER=ollama
LLM_TEMPERATURE=0.2
```

> **Note:** Ensure the `MONGOBD_USERNAME` and `MONGODB_PASSWORD` in `.env` match the values used in `docker-compose.yml`.

### Running the Application

**Start the FastAPI backend:**

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Start the Gradio frontend** (in a separate terminal):

```bash
python -m src.frontend.gradio_app
```

**Access the application:**

| Service | URL |
|---|---|
| Gradio Web UI | [http://localhost:7860](http://localhost:7860) |
| FastAPI Docs (Swagger) | [http://localhost:8000/docs](http://localhost:8000/docs) |
| LangServe Playground | [http://localhost:8000/chat/playground](http://localhost:8000/chat/playground) |
| Qdrant Dashboard | [http://localhost:6333/dashboard](http://localhost:6333/dashboard) |

---

## üì° API Reference

### Data Routes ‚Äî `/data`

#### `POST /data/upload`

Upload one or more documents and associate them with a chat session.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `files` | `UploadFile[]` | ‚úÖ | Files to upload (PDF, DOCX, TXT) |
| `chat_id` | `string` (Form) | ‚úÖ | Target chat session identifier |
| `persona_name` | `string` (Form) | ‚ùå | Persona name (default: `"General Assistant"`) |
| `persona_instructions` | `string` (Form) | ‚ùå | System instructions for the AI persona |

<details>
<summary><b>Example Response</b></summary>

```json
{
  "message": "Successfully uploaded 2 files.",
  "uploaded": [
    { "filename": "research_paper.pdf", "file_id": "a1b2c3d4e5f6_research_paper.pdf" },
    { "filename": "notes.txt", "file_id": "g7h8i9j0k1l2_notes.txt" }
  ],
  "failed": []
}
```
</details>

#### `POST /data/process`

Process (chunk, embed, and index) all pending files for a given chat session.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `chat_id` | `string` (JSON) | ‚úÖ | Chat session to process files for |

<details>
<summary><b>Example Response</b></summary>

```json
{
  "message": "Processing complete. Success: 2, Failed: 0",
  "processed": ["a1b2c3d4e5f6_research_paper.pdf", "g7h8i9j0k1l2_notes.txt"],
  "failed": []
}
```
</details>

### Chat Routes ‚Äî `/chat`

#### `POST /chat/invoke` (LangServe)

Send a message to the RAG chain and receive an AI-generated response with citations.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `input.question` | `string` | ‚úÖ | The user's question |
| `input.chat_id` | `string` | ‚úÖ | Active chat session identifier |
| `input.persona` | `string` | ‚ùå | Custom persona override |

<details>
<summary><b>Example Request</b></summary>

```json
{
  "input": {
    "question": "What are the key findings in chapter 3?",
    "chat_id": "abc123"
  }
}
```
</details>

#### `GET /chat/history`

Returns a list of all chat session IDs.

#### `GET /chat/history/{chat_id}`

Returns the full message history for a specific chat session.

---

## üìÅ Project Structure

```
VersaGraph-RAG-Agent/
‚îú‚îÄ‚îÄ main.py                          # FastAPI application entry point & lifespan management
‚îú‚îÄ‚îÄ docker-compose.yml               # Qdrant + MongoDB infrastructure
‚îú‚îÄ‚îÄ pyproject.toml                   # Project metadata & dependencies
‚îú‚îÄ‚îÄ .env                             # Environment variables (not committed)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chains/                  # LangChain RAG pipeline
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_chain.py         # Full RAG chain assembly (rephrase ‚Üí retrieve ‚Üí generate)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py         # Dual retriever (documents + conversation history)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py           # System & user prompt templates
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_manager.py    # Hybrid memory (MongoDB + Qdrant) manager
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ LLMProvider.py   # Multi-provider LLM factory (Groq / Ollama)
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ LLMProviderEnums.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mongo_db/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BaseDataModel.py # Abstract base with auto-indexing
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatModel.py     # Chat session CRUD & file registry
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileModel.py     # File metadata CRUD
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageModel.py  # Message persistence & retrieval
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DataBaseEnum.py  # Collection names & file status enums
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Chats.py     # Chat & ChatFile Pydantic models
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Files.py     # File Pydantic model
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Messages.py  # Message Pydantic model
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qdrantdb/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ QdrantdbModel.py # Qdrant collection management, upsert & search
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py              # Chat endpoints & LangServe registration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.py              # File upload & processing endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ProcessRequest.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utilities/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ process_file.py      # File validation, storage & metadata extraction
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ loader.py            # Multi-format document loading (PDF, DOCX, TXT)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ splitter.py          # Recursive text chunking
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ embeder.py           # Ollama-based text embedding
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ProcessEnum.py       # Processing signal enums
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ files/                   # Uploaded file storage directory
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gradio_app.py            # Gradio web interface
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ helper/
‚îÇ       ‚îî‚îÄ‚îÄ config.py                # Pydantic Settings with .env integration
```

---

## üîÑ How It Works

### 1. Document Ingestion Pipeline

```
Upload File(s) ‚Üí Validate (type, size) ‚Üí Save to Disk ‚Üí Store Metadata in MongoDB
                                                              ‚îÇ
                                                              ‚ñº
Process Request ‚Üí Load Document ‚Üí Split into Chunks ‚Üí Generate Embeddings ‚Üí Upsert to Qdrant
                  (PyMuPDF/       (Recursive            (Ollama             (with chat_id
                   Docx2txt/       Character              Embeddings)         filter payload)
                   TextLoader)     Splitter)
```

1. **Upload**: Files are validated against allowed types and size limits, then saved to disk with unique identifiers.
2. **Metadata Tracking**: File records are created in MongoDB with status tracking (`uploaded` ‚Üí `processing` ‚Üí `indexed`).
3. **Processing**: Documents are loaded, split into overlapping chunks, embedded into vectors, and stored in Qdrant with `chat_id` filtering metadata.

### 2. RAG Query Pipeline

```
User Question
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fetch Chat History  ‚îÇ‚îÄ‚îÄ> MongoDB (last 6 messages)
‚îÇ Fetch File Names    ‚îÇ‚îÄ‚îÄ> MongoDB (files in session)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Rephrase Question   ‚îÇ‚îÄ‚îÄ> LLM reformulates with context into standalone query
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Dual Retrieval      ‚îÇ
‚îÇ  ‚Ä¢ Document chunks  ‚îÇ‚îÄ‚îÄ> Qdrant (top 10 by similarity, filtered by chat_id)
‚îÇ  ‚Ä¢ History memories ‚îÇ‚îÄ‚îÄ> Qdrant (top 2 from conversation history)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Generate Answer     ‚îÇ‚îÄ‚îÄ> LLM produces cited response using retrieved context
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Persist Turn        ‚îÇ‚îÄ‚îÄ> Save user msg + AI response to MongoDB & Qdrant
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3. Hybrid Memory System

VersaGraph implements a dual-memory architecture:

- **Short-Term Memory**: The last N messages retrieved from MongoDB, providing immediate conversational context to the LLM.
- **Long-Term Semantic Memory**: All conversation turns are embedded and stored in Qdrant. During retrieval, semantically relevant past exchanges are surfaced alongside document results ‚Äî enabling the agent to recall relevant information from much earlier in the conversation.

---

## ‚öôÔ∏è Configuration

All configuration is managed through environment variables via Pydantic Settings (`.env` file).

### File Processing

| Variable | Type | Description |
|---|---|---|
| `FILE_ALLOWED_TYPES` | `list[str]` | Accepted MIME types for upload |
| `FILE_MAX_SIZE_MB` | `int` | Maximum file size in megabytes |
| `FILE_UPLOAD_CHUNK_SIZE` | `int` | Async file write buffer size (bytes) |

### Text Chunking

| Variable | Type | Description |
|---|---|---|
| `CHUNK_SIZE` | `int` | Maximum characters per chunk |
| `CHUNK_OVERLAP` | `int` | Overlap between consecutive chunks |

### Vector Store (Qdrant)

| Variable | Type | Description |
|---|---|---|
| `EMBEDDING_MODEL` | `str` | Ollama embedding model name |
| `EMBED_MODEL_SIZE` | `int` | Embedding vector dimensionality |
| `DISTANCE_METRIC` | `str` | Similarity metric (`Cosine`, `Euclid`, `Dot`) |
| `URL_QDRANT` | `str` | Qdrant server URL |
| `COLLECTION_APP_NAME` | `str` | Collection for document vectors |
| `COLLECTION_CHATS_HISTORY_NAME` | `str` | Collection for conversation history vectors |

### LLM Provider

| Variable | Type | Description |
|---|---|---|
| `LLM_PROVIDER` | `str` | `"groq"` or `"ollama"` |
| `LLM_MODEL` | `str` | Model identifier (e.g., `llama-3.3-70b-versatile`) |
| `API_KEY_GROQ` | `str` | Groq API key (required for Groq provider) |
| `API_URL_LLM` | `str` | Ollama server URL (required for Ollama provider) |
| `LLM_TEMPERATURE` | `float` | Response randomness (0.0 ‚Äì 1.0) |

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'feat: add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

Please ensure your code follows the existing project structure and includes appropriate documentation.

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see the [LICENSE](LICENSE) file for details.

```
MIT License ‚Ä¢ Copyright (c) 2026 Mahmoud El Saeed Mohammed
```

---

<div align="center">

**Built with ‚ù§Ô∏è using LangChain, FastAPI, Qdrant, and MongoDB**

</div>
